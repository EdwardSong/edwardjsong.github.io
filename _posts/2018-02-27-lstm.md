---
layout: default
title: My LSTM Network Notes - Intro to LTSMs
tags: [AI, ML, Artificial Intelligence, Machine Learning]
permalink: /machine-learning/long-short-term-memory-networks/intro
category: Work
---

## My LSTM Network Notes - Intro to LTSMs 
A deep learning technique designed specifically for sequence prediction problems.  A type of Recurrent Neural Network (RNN) is an RNN architecture specifically designed to address the vanishing gradient problem.
### Terminology

<ul>
    <li>Sequence prediction attempts to predict elements of a sequence on the basis of the preceding elements</li>
    <li>Multilayer Perceptrons (MLP's) - classical neural networks, which can be applied to sequence prediction problems.  They can approximate a mapping function from input variables to output variables over time, which lends them to forecasting, especially when it comes to the Vanishing Gradient Problem.</li>
    <li>Recurrent Neural Network - special type of Neural Network designed for sequencing problems.  Given a standard feedforward MLP network, an RNN can be thought of as the addition of loops to the architecture.  The recurrent connections add state or memory to the network and allow it to learn and harness the ordered nature of observations within input sequences.</li>
    <li>Vanishing Gradient Problem - training experiments show how difficult this was where the weight update procedure resulted in weight changes that quickly became so small as to have no effect (vanishing gradients) or so large as to result in very large changes or even overflow (exploding gradients)</li>    
</ul>

### Types

<ul>
    <li><strong>Vanilla LSTM</strong> - memory cells of a single LSTM layer are used in a simple network structure.</li>
    <li><strong>Stacked LSTM</strong> - LSTM layers are stacked one on top of another into deep networks.</li>
    <li><strong>CNN LSTM</strong> - A convolutional neural network is used to learn features in spatial input like images and the LSTM can be used to support a sequence of images as input or generate a sequence in response to an image.</li>
    <li><strong>Encoder-Decoder LSTM</strong> - One LSTM network encodes input sequences and a separate LSTM network decodes the encoding into an output sequence.</li>
    <li><strong>Bidirectional LSTM</strong> - Input sequences are presented and learned both forward and backward.</li>
    <li><strong>Generative LSTM</strong> - LSTMs learn the structure relationship in input sequences so well that they can generate new plausible sequences.</li>
</ul>

### Notes
<ul>
    <li>LTSM &gt; MLPs</li>
    <li>Training LTSM's &gt; Training RNN</li>
    <li>Context info in LTSM's &gt; Context info in RNN</li>
    <li>In time series forecasting, often the information relevant for making a forecast is within a small window of past observations. Often an MLP with a window or a linear model may be a less complex and more suitable model.</li>
    <li>An important limitation of LSTMs is the memory. Or more accurately, how memory can be abused. It is possible to force an LSTM model to remember a single observation over a very long number of input time steps. This is a poor use of LSTMs and requiring an LSTM model to remember multiple observations will fail.</li>  
</ul>

### Further Reading

<ul>
    <li>http://ArXiv.org</li>
    <li>https://scholar.google.com/</li>
</ul>

[back](./)